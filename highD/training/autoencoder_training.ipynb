{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Autoencoder training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import h5py\n",
    "import keras_preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "\n",
    "from PIL import Image\n",
    "from sys import platform\n",
    "from tensorflow import keras\n",
    "\n",
    "sns.set()\n",
    "sns.set_style(\"whitegrid\")\n",
    "sns.set_context(\"paper\")\n",
    "sns.color_palette(\"hls\", 8)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_random_hdf5_files(directory, n = None):\n",
    "    \"\"\"\n",
    "    Load random HDF5 files containing images from a specified directory and concatenate them into a single NumPy array.\n",
    "\n",
    "    Args:\n",
    "        directory (str): The path to the directory containing HDF5 files.\n",
    "        n (int): The number of random files to load.\n",
    "\n",
    "    Returns:\n",
    "        train_data: A  NumPy array containing all the images from the train dataset of the loaded HDF5 files.\n",
    "        validation_data: A single NumPy array containing all the images from the validation dataset of the loaded HDF5 files.\n",
    "        test_data: A single NumPy array containing all the images from the the test dataset of the loaded HDF5 files.\n",
    "\n",
    "    The function selects 'n' random HDF5 files from the specified 'directory', reads the datasets from each\n",
    "    file, and concatenates these datasets into a single NumPy array. This array contains all the images from the loaded\n",
    "    HDF5 files. The function returns three arrays, one each for training, validation and test.\n",
    "\n",
    "    Note:\n",
    "    - Ensure that the HDF5 files in the directory have datasets named 'train', 'validation', and  'test' (or adjust accordingly).\n",
    "    - If 'n' is greater than the number of available HDF5 files in the directory, all available files will be loaded.\n",
    "    - if 'n' is not specified, all available files will be loaded\n",
    "\n",
    "    Example usage:\n",
    "    >>> directory_path = '/path/to/your/directory'\n",
    "    >>> n = 3\n",
    "    >>> train_data, val_data, test_data = load_random_hdf5_files(directory_path, n_files_to_load)\n",
    "    \"\"\"\n",
    "\n",
    "    file_list = []\n",
    "    # List all HDF5 files in the directory\n",
    "    for filename in os.listdir(directory):\n",
    "        if filename.endswith(\".hdf5\"):\n",
    "            file_list.append(os.path.join(directory, filename))\n",
    "    if not file_list:\n",
    "        print(\"No HDF5 files found in the specified directory.\")\n",
    "        return None\n",
    "    if (n > len(file_list)) or (n is None):\n",
    "        print(f\"Requested to load {n} files, but there are only {len(file_list)} files available.\")\n",
    "        n = len(file_list)\n",
    "\n",
    "    # Randomly select n files\n",
    "    random_files = random.sample(file_list, n)\n",
    "    train_data = []\n",
    "    validation_data = []\n",
    "    test_data = []\n",
    "    for file_path in random_files:\n",
    "        with h5py.File(file_path, 'r') as hf:\n",
    "            d1 = np.array(hf[\"train\"])\n",
    "            d2 = np.array(hf[\"validation\"])\n",
    "            d3 = np.array(hf[\"test\"])\n",
    "            train_data.append(np.array(d1))\n",
    "            validation_data.append(np.array(d2))\n",
    "            test_data.append(np.array(d3))\n",
    "    # Concatenate the individual arrays into a single NumPy array\n",
    "    train_data = np.concatenate(train_data, axis=0)\n",
    "    validation_data = np.concatenate(validation_data, axis=0)\n",
    "    test_data = np.concatenate(test_data, axis=0)\n",
    "\n",
    "    return train_data, validation_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/wd/m_272rg54lgflvcghpr2vnsc0000gn/T/ipykernel_6949/2745363869.py:48: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  d1 = np.array(hf[\"train\"])\n",
      "/var/folders/wd/m_272rg54lgflvcghpr2vnsc0000gn/T/ipykernel_6949/2745363869.py:49: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  d2 = np.array(hf[\"validation\"])\n",
      "/var/folders/wd/m_272rg54lgflvcghpr2vnsc0000gn/T/ipykernel_6949/2745363869.py:50: DeprecationWarning: `product` is deprecated as of NumPy 1.25.0, and will be removed in NumPy 2.0. Please use `prod` instead.\n",
      "  d3 = np.array(hf[\"test\"])\n"
     ]
    }
   ],
   "source": [
    "if platform == 'darwin':\n",
    "    images_path = \"/Users/lmiguelmartinez/Tesis/datasets/highD/images_1000ms\"\n",
    "else:\n",
    "    images_path = \"/home/lmmartinez/Tesis/datasets/highD/images_1000ms\"\n",
    "\n",
    "train_data, val_data, test_data = load_random_hdf5_files(images_path, n = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = train_data[0].shape\n",
    "BATCH_SIZE = 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(121, 201)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apr_aut",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
